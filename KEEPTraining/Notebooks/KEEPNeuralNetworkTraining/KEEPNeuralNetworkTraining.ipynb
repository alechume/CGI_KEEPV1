{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Enhancement and Evaluation Project Neural Network\n",
    "<b>This notebook contains the necessary steps and descriptions to train a neural network for assessing the quality of tickets.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import random\n",
    "from os import path\n",
    "\n",
    "import numpy\n",
    "from fastai.text import *\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame, read_excel, read_sql\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyodbc import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "This cell gets all the tickets from Jira and puts them into a Pandas dataframe. This dataframe then creates a new column that combines both the description and resolution columns into a single column.\n",
    "\n",
    "This cell will probably need to be changed to fit the data source. The neural network expects one column called Combined however this can be changed to multiple columns if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connect('Driver={SQL Server};Server=CA-L7XNYHR2\\SQLEXPRESS;Database=jiradb_UAT;Trusted_Connection=yes;')\n",
    "query = '''\n",
    "SELECT jiss.ID, jiss.DESCRIPTION as Description, TEXTVALUE as Resolution\n",
    "FROM [jiradb_UAT].[jiraschema].[jiraissue] jiss\n",
    "LEFT JOIN  [jiradb_UAT].[jiraschema].[issuetype] isstype\n",
    "ON jiss.issuetype = isstype.ID \n",
    "LEFT JOIN [jiradb_UAT].[jiraschema].[customfieldvalue] CFV \n",
    "ON jiss.ID = CFV.ISSUE\n",
    "WHERE CFV.CUSTOMFIELD = 10801 AND jiss.DESCRIPTION IS NOT NULL AND TEXTVALUE IS NOT NULL AND NOT isstype.pstyle IS NULL\n",
    "'''\n",
    "queried_dataframe = read_sql(query,connection)\n",
    "queried_dataframe = queried_dataframe.fillna(' ')\n",
    "queried_dataframe[\"Combined\"] = str(queried_dataframe[\"Description\"]) + ' ' + str(queried_dataframe[\"Resolution\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional</b> This cell applies some regular expression to the text to remove irrelevant information that occurs fairly frequently in the set. Modify this to your own dataset or skip entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_dataframe[\"Combined\"] = queried_dataframe[\"Combined\"].replace({r'\\s+$':'',r'^\\s+':'',r'Incident No:(.*?)Notes:':'',r'[^A-Za-z]':' ',r'From:(.*?)Description:':'',r'Name:(.*?)Issue:':'',r'Ministry:(.*?)Issue:':''}, regex=True).replace({r'\\n':' ',r'\\t':' '}, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Language Model\n",
    "\n",
    "This cell creates the databunch used by the language model learner and creates the language model learner from the databunch. The databunch is created first by making a textlist from the text in the column that was created in the dataframe. This textlist is split randomly so that 10% is used for validation and the other 90% is used for training. The text list is then labelled to indicate this is going the be used for a language model and then transforms the text list into a databunch with a batch size as an argument.\n",
    "\n",
    "Creating the language model takes the databunch we created above to create the dictionary and the pretrained model. The pretrained model I am using is the AWD-LSTM which stands for Average Stochastic Gradient Descent Weight Dropped Long Short Term Memory. This pretrained model is a recurrent neural network which is used as it allows it to learn from a sequence in this case sentences and documents. The final argument is the dropout multiplier which determine how much dropouts to use. This model uses embedding dropout, input dropout, weight dropout and hidden dropout.\n",
    "\n",
    "**Relevant Information**\n",
    "- [DataBunch](https://docs.fast.ai/basic_data.html#DataBunch)\n",
    "- [DataBlock API](https://docs.fast.ai/data_block.html)\n",
    "- [AWD-LSTM](https://arxiv.org/abs/1708.02182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=30\n",
    "validation_percent=0.1\n",
    "dropout_lm=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = (TextList.from_df(queried_dataframe, cols=\"Combined\").use_partial_data(0.01).split_by_rand_pct(validation_percent).label_for_lm().databunch(bs=batchsize))\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=dropout_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used to determine the proper learning rate by cross validation. This tries multiple learning rates and determines the loss from each of these.\n",
    "\n",
    "The best learning rate is as large as possible without being so large as to cause the model to be unable to converge. The learning rate doesn't matter very much as long as it is somewhat close to optimal everything will work fine.\n",
    "\n",
    "<img src=\"./images/lr1.png\"/>\n",
    "This image shows a standard learning rate finder curve; The best learning rate from this type of curve is near the steepest decline on the graph. I would choose around 2e-2 but as long as you are within 1e-2 and 1e-1 for this curve you wouldn't notice much of a difference.\n",
    "\n",
    "<img src=\"./images/lr2.png\"/>\n",
    "This image is a less common learning rate finder curve; This curve is harder to interpret but still fairly simple. Look for the point before the curve starts to increase in this case around 1e-4 divide that learning rate by 10. This gives the learning rate of 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='33' class='' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      97.06% [33/34 03:04<00:05]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.045751</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.021335</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.021009</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.015361</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.013792</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.012055</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.013147</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.009028</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.008722</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.007843</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.005923</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.000422</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.996761</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.991733</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.982171</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.963582</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.935635</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.889203</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.812616</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.696094</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.533791</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.324497</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.076737</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.807665</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.542131</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.290730</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.060913</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.847913</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.656868</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.479364</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.318598</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.195225</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.265528</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the language model using the one cycle policy on the frozen model.\n",
    "```\n",
    "learn.fit_one_cycle(epochs, learning rate, momentums)\n",
    "```\n",
    "\n",
    "The one cycle policy consists of adjusting momentums and learning rate as you train. Momentums are used to keep moving in the same direction as before and learning rate is the size of the step that is made. The one cycle policy starts by using high momentum and low learning rate then reducing the momentums and increasing the learning rate until you hit your max learning rate and then decreasing the learning rate and increasing the momentums.\n",
    "\n",
    "A frozen model simply means we are holding the weights of every layer beside the last constant and training only the last layer.\n",
    "\n",
    "The number of epochs is the amount of times the model will be trained over the whole dataset. After each batch which is defined by the batchsize in the databunch we update the model from the results. The number of epochs doesn't have much affect as if you need to train more then just run it again (Don't do it 1 epoch at a time though or you will apply the one cycle policy too much). I usually use 4-5 epochs and determine if I need more after that since the model is frozen it doesn't take many epochs to optimize the last layer.\n",
    "\n",
    "The learning rate is simply the result of the learning rate finder.\n",
    "\n",
    "The momentums don't need to be adjusted as they are quite conservative and adjusting them only makes it slightly faster to converge but also more likely to be unable to converge. The defaults are (0.95,0.85)\n",
    "\n",
    "Keep training the model until the training loss is lower than the validation loss and the validation loss is increasing. This means that the model is starting to overfit so training anymore is only going to make the model worse. The accuracy at this stage does not matter as the model is trying to predict the next word in a sentence and the classifier will always have a much higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_langmodel = 6e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.496082</td>\n",
       "      <td>2.212031</td>\n",
       "      <td>0.521905</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.976170</td>\n",
       "      <td>0.598416</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.080879</td>\n",
       "      <td>0.304245</td>\n",
       "      <td>0.918571</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.573670</td>\n",
       "      <td>0.260951</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, learning_rate_langmodel, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is unfrozen training affects every layer. The earlier layers are not going to change as much as the last layers which is why we trained those first and since we don't want to overfit the model we are going to reduce the learning rate for the unfrozen model. I always divide the previous learning rate in half for the new learning rate.\n",
    "\n",
    "This stage is going to take longer to train and also requires more epochs until it starts to overfit. I normally do between 5-9 epochs. If the accuracy goes down from the previous step do not worry this is normal and is why accuracy doesn't matter. Train this until training loss is lower than validation loss and validation loss is increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.427956</td>\n",
       "      <td>0.648970</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.448040</td>\n",
       "      <td>1.089996</td>\n",
       "      <td>0.735238</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.588504</td>\n",
       "      <td>1.235270</td>\n",
       "      <td>0.760952</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.616388</td>\n",
       "      <td>0.291715</td>\n",
       "      <td>0.929524</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506867</td>\n",
       "      <td>0.254816</td>\n",
       "      <td>0.945714</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.424196</td>\n",
       "      <td>0.176646</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.361422</td>\n",
       "      <td>0.181322</td>\n",
       "      <td>0.959048</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.313842</td>\n",
       "      <td>0.167741</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.277131</td>\n",
       "      <td>0.166559</td>\n",
       "      <td>0.960952</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(9, learning_rate_langmodel/2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the language model is finished training we need to export the encoder. This encoder can **not** be loaded and trained as a language model after it has been exported use the save function instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('./encoders/Language_Model_Encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating the Classifier\n",
    "\n",
    "Load the labelled data into a pandas dataframe and combine them into one column like for the language model. This cell may need to be rewritten depending on how the labelled data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labelled_dataframe = read_excel('./data/labelled.xlsx').dropna().astype({\"ID\":int,\"Quality\":int})\n",
    "labelled_dataframe[\"Combined\"] = labelled_dataframe[\"DESCRIPTION\"] + ' ' + labelled_dataframe[\"TEXTVALUE\"]\n",
    "labelled_dataframe.Combined = labelled_dataframe.Combined.replace({r'\\s+$':'',r'^\\s+':'',r'Incident No:(.*?)Notes:':'',r'[^A-Za-z]':' ',r'From:(.*?)Description:':'',r'Name:(.*?)Issue:':'',r'Ministry:(.*?)Issue:':''}, regex=True).replace({r'\\n':' ',r'\\t':' '}, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Optional** If the data is heavily imbalanced between classes then oversampling the minority class can improve the results.\n",
    "\n",
    "This cell splits the dataframe into training data and duplicates the minority class until equal and then creates a databunch from the  new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(labelled_dataframe.Combined, labelled_dataframe.Quality, test_size=validation_percent)\n",
    "train = DataFrame({'Combined':X_train,'Quality':Y_train})\n",
    "test = DataFrame({'Combined':X_test,'Quality':Y_test})\n",
    "minority_class = numpy.argmin(train.groupby(\"Quality\").size())\n",
    "majority_class = numpy.argmax(train.groupby(\"Quality\").size())\n",
    "newdf = DataFrame(numpy.repeat(train[train.Quality==minority_class].values,math.floor(train.groupby(\"Quality\").size()[majority_class]/train.groupby(\"Quality\").size()[minority_class])-1,axis=0))\n",
    "newdf.columns = train.columns\n",
    "train = train.append(newdf)\n",
    "data_classifier = TextClasDataBunch.from_df('',train,test,text_cols=0,label_cols=1,bs=batchsize, vocab=data_lm.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Important\n",
    "\n",
    "Only run this if you are not oversampling using the above cell.\n",
    "\n",
    "This creates the databunch for the classifier from the labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_classifier = (TextList.from_df(labelled_dataframe, vocab=data_lm.vocab, cols=\"Combined\").split_by_rand_pct(validation_percent).label_from_df(cols=\"Quality\").databunch(bs=batchsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have a trained model that can predict the next word in a sentence we want to adapt this to label tickets as low or high quality. We create a new model with the labelled data and the same architecture however without the pretrained weights then we load the weights from the model that we trained above. This new model does not contain the decoder which is used to predict the next word instead we have only the encoder which evaluates the previous words. We train this new model to classify the tickets.\n",
    "\n",
    "Once we have created the new model we freeze the layers so only the last layer can change and find the correct learning rate the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dropout_class=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 03:20<03:20]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.796110</td>\n",
       "      <td>#na#</td>\n",
       "      <td>03:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='47' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      94.00% [47/50 02:56<00:11 2.5445]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_classifier = text_classifier_learner(data_classifier, AWD_LSTM, drop_mult=dropout_class, pretrained=False, metrics=accuracy)\n",
    "learn_classifier.load_encoder('./encoders/Language_Model_Encoder')\n",
    "learn_classifier.freeze()\n",
    "learn_classifier.lr_find()\n",
    "learn_classifier.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training The Classifier\n",
    "\n",
    "When training the classifier we unfreeze the model gradually and slow down the learning rate as we unfreeze more layers. This method allows us to quickly adjust the last layers from predicting a word to predicting the quality without affecting the underlying knowledge learned from the language model. Once the last layer of the classifier is optimized we unfreeze the next layer and allow the model to fine tune the deeper knowledge learned previously.\n",
    "\n",
    "The learning rate is adjusted so that when only the last layer is unfrozen that layer is trained with the given learning rate but once we start unfreezing the layers we logarthimically distribute different learning rates for each layer. This causes us to not affect the first layers as much as the last but still fine tune to the correct results. The value of ${2.6}^4$ is the best value for the starting point from empirical evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learning_rate_classifier = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.735648</td>\n",
       "      <td>0.536973</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.671513</td>\n",
       "      <td>0.652237</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>04:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.631296</td>\n",
       "      <td>0.557402</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>04:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "learn_classifier.fit_one_cycle(3, learning_rate_classifier, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn_classifier.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.618279</td>\n",
       "      <td>0.387628</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>05:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.630932</td>\n",
       "      <td>0.453106</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>05:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.600661</td>\n",
       "      <td>0.537208</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563558</td>\n",
       "      <td>0.594985</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>05:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "learn_classifier.fit_one_cycle(4,slice(learning_rate_classifier/(2.6**4),learning_rate_classifier), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn_classifier.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.534610</td>\n",
       "      <td>0.446660</td>\n",
       "      <td>0.786517</td>\n",
       "      <td>13:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.534924</td>\n",
       "      <td>0.460531</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>14:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.540055</td>\n",
       "      <td>0.371624</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>13:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.490244</td>\n",
       "      <td>0.283569</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>12:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.458423</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>11:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.453750</td>\n",
       "      <td>0.409887</td>\n",
       "      <td>0.842697</td>\n",
       "      <td>11:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.346742</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>13:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.415618</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.865169</td>\n",
       "      <td>11:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "learn_classifier.fit_one_cycle(8,slice((learning_rate_classifier/2)/(2.6**4),learning_rate_classifier/2),moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once the model is finished training export the model so that it can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn_classifier.export(\"./models/KEEPModel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner = load_learner('./models', 'KEEPModel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_multiline = '''\n",
    "SUBTASK:Initial Response on Incident.\n",
    "ITSM Ticket:INC000002369276\n",
    " Received Date:2017-07-17 09:31:00.0\n",
    " RESPONSE DUE BY:2017-07-17 10:31:00.0\n",
    " RESPONDED BY:2017-07-17 12:16:00.0\n",
    " TASK NOTES:Incident No:INC000002369276\n",
    "\n",
    "Date: 2017-07-17T12:16:00.000-0600\n",
    "\n",
    "Type: General Information\n",
    "\n",
    "Summary: Please add Catherine.Eic\n",
    "\n",
    "Notes: Please add Catherine.Eichenlaub in groups.nsf for the below group.\n",
    "\n",
    "CSI BIM Editors\n",
    "\n",
    "user added to application group and database\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\torch_core.py:83: UserWarning: Tensor is int32: upgrading to int64; for better performance use int64 input\n",
      "  warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n",
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\text\\data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  idx_min = (t != self.pad_idx).nonzero().min()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.8247, 0.1753]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(test_text_multiline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_singleline = 'SUBTASK:Initial Response on Incident. ITSM Ticket:INC000002369276 Received Date:2017-07-17 09:31:00.0 RESPONSE DUE BY:2017-07-17 10:31:00.0 RESPONDED BY:2017-07-17 12:16:00.0 TASK NOTES:Incident No:INC000002369276 Date: 2017-07-17T12:16:00.000-0600 Type: General Information Summary: Please add Catherine.Eic Notes: Please add Catherine.Eichenlaub in groups.nsf for the below group. CSI BIM Editors user added to application group and database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\torch_core.py:83: UserWarning: Tensor is int32: upgrading to int64; for better performance use int64 input\n",
      "  warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.6580, 0.3420]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(test_text_singleline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBTASK Initial Response on Incident  ITSM Ticket INC             Received Date                       RESPONSE DUE BY                       RESPONDED BY                       TASK NOTES  Please add  Catherine Eichenlaub  in groups nsf for the below group  CSI BIM Editors user added to application group and database\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_text_regex = test_text_singleline\n",
    "test_text_regex = re.sub(r'\\s+$', '', test_text_regex)\n",
    "test_text_regex = re.sub(r'^\\s+', '', test_text_regex)\n",
    "test_text_regex = re.sub(r'Incident No:(.*?)Notes:', '', test_text_regex)\n",
    "test_text_regex = re.sub(r'[^A-Za-z]', ' ', test_text_regex)\n",
    "test_text_regex = re.sub(r'From:(.*?)Description:', '', test_text_regex)\n",
    "test_text_regex = re.sub(r'Name:(.*?)Issue:', '', test_text_regex)\n",
    "test_text_regex = re.sub(r'Ministry:(.*?)Issue:', '', test_text_regex)\n",
    "print(test_text_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\torch_core.py:83: UserWarning: Tensor is int32: upgrading to int64; for better performance use int64 input\n",
      "  warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.9182, 0.0818]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(test_text_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_quality1 = 'Investigate why the $ are out of balance with RCVAging Cube but the Person # is correct I have completed the investigation for the discrepancy between the RCV Aging cube and the Special Accounts report. This discrepancy was investigated back in July 2018 (as per the attached email). Separate emails for August and Sept 2018 were not sent since the same explanation applied. Once again for October the same explanation is relevant for the difference. Going forward, no tickets will be created if the difference is due to the same explanation already reported. Let me know if you would prefer a ticket be created each month providing you with the same explanation. Explanation: The coding for the Special Accounts report does not accommodate when a person has a credit outstanding amount that has a different last activity date/time than other transactions with outstanding amounts > 0. The Special Accounts report totals all the outstanding balances for a person based on the last activity date/time. If the total outstanding amount is greater than zero then the person and the data is reported on the Special accounts report. For the month of July, there were some total outstanding amounts with credit balances that were not included in the Special accounts report because the total outstanding amount for the Last activity date/time was negative. Consideration to make coding changes for The Special Accounts report. Subtotal the outstanding amounts based on the last_activity_date instead of the last activity date + time. In addition, select all outstanding balance totals, not only the totals that are > 0. If these coding changes are acceptable, they could be included as part of  SR9387 to convert the Special Accounts report.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alec.hume\\documents\\keep local installation\\keep training\\keeptraining-env\\lib\\site-packages\\fastai\\torch_core.py:83: UserWarning: Tensor is int32: upgrading to int64; for better performance use int64 input\n",
      "  warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Category tensor(0), tensor(0), tensor([0.5195, 0.4805]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(test_text_quality1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What does a Neural Network do when Training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal of the neural network in KEEP is to be able to classify the text given as low or high quality based on the labelled data that is given to it. A neural network learns to classify the labelled data as high or low quality but to ensure that we are actually learning what makes the text high quality we need to make sure the neural network isn't just learning the labels of our data. This is done by splitting our labelled data into a training set and a validation set so that the model only learns off the data in the training set and we can ensure the model has actually learned what we wanted with the validation set.\n",
    "\n",
    "We can validate that the neural network is learning but to actually teach the neural network is harder. To train the model we need some method to determine how to improve the model; the solution to this is a loss function. A loss function is how we determine how incorrect a model was on its prediction, for example if a model is incorrect with 99% confidence we want to punish that much more than being incorrect with a 55% confidence. The loss function that is used for classification is called cross entropy loss and it helps tell us that the model is getting better and also how to make the model better. The loss function when minimized is the best that this model can be trained for the given data so we train the model by minimizing the loss function.\n",
    "\n",
    "Now we know how the model learns but how does the model actually apply what it learns to itself? First we need to know what the inside of a neural network actually looks like. A neural network much like the name suggests is a network of neurons, these neurons are structured into layers with 3 different types of layers called input, output and hidden. Every neuron in the hidden layers takes the results of every neuron of the previous layer and sends its result to every neuron in the next layer. The input and output layers are different because the input layer only sends its result to every neuron in the next layer and the output layer only receives the results of every neuron in the previous layer. This is all good but what does a singular neuron actually do when it receives this information? A neuron consists of weights, a bias and an activation function. The weights and bias are adjusted during training and the activation function doesn't change ever. A neuron's weights is multiplied by the inputs of the neuron so the larger a weight the more important that neuron is on classifying the data. A neuron's bias is added after the inputs have been multiplied by the weights and the bias determines at what point the result of the neuron becomes useful. The results are then fed into the activation function before being sent to the next layer; the activation function that we use is a version of ReLu called Leaky Relu and is important as without an activation function the model could only create a linear classifier. A ReLu is very simple and either outputs the input value if greater than 0 and outputs 0 for any input less than 0. This does have a problem though called neuron death where a neuron could train to a weight and bias that always outputs 0 for any input so we solve this using a Leaky ReLu. A Leaky ReLu has the same output for positive numbers but for negative numbers instead of outputting 0 it multiples them by a very small constant which won't affect the results of the next layer much but prevents the neuron from dying.\n",
    "<img src=\"./images/nn.jpeg\"/>\n",
    "\n",
    "We have learned what a loss function is and the structure of a neural network so now we can see how to update the weights and biases to minimize the loss function. This process is called backpropagation which consists of going through each neuron from back to front and determining how to update them. If we assume that the neuron of our output layer says that the text we feed through the model is low quality when we said it was high quality then we want to update the model to say this text is high quality so we calculate the loss and then look at the previous layer of the model. We want to decrease the values sent to the low quality output neuron so we look at the results of the previous layer and reduce the weight and/or increase the bias proportional to the result of the neuron and the loss function. We also do the same for the high quality output neuron but instead we want to increase the values sent so we increase the weight and/or decrease the bias proportional to the result of the neuron and the loss function. We do this for every neuron in the network. What we are doing during backpropagation is called gradient descent. A gradient is a multivariable generalization of a derivative so if we remember that the derivative tells us the rate of change for a point the gradient is a vector of derivatives where each value in the vector is the rate of change for each input. The gradient is a vector which means that it has a direction which is the direction of greatest increase at the point on the loss function so the negative is the greatest decrease at the point on the loss function. If we calculate the gradient for a batch of inputs and take the average then we have the direction of average greatest decrease for the loss function.\n",
    "\n",
    "The neural network described above isn't the architecture that KEEP is using. The above describes a feed forward neural network which means all neurons connect only to the layer ahead of them but other architectures exist such as Convolutional Neural Network (CNN) which is used for images to get information from groups of inputs. The architecture that KEEP uses is called AWD-LSTM but before we can understand AWD-LSTM we need to know about recurrent neural networks or RNN. RNNs are the same as feed forward neural networks but the output of the network connects to the input of the network. This idea may seem confusing but it makes sense if we think about using a sentence as input. If we used a feed forward network then we would either have to feed each word one at a time which means we wouldn't have context from previous words or feed the whole sentence at once which wouldn't know the order of the sentence. Recurrent neural networks still have the results from their previous state so we retain information about the sentence that can't be captured with feed forward networks. In practice though RNNs can only contain so much information with one connection so holding information for a long sentence or document is unfeasible. A special type of RNNs are able to capture this information called LSTM or Long Short Term Memory which is quite a bit more complicated than just connecting to itself. An LSTM consists of 4 different neural networks.\n",
    "<img src=\"./images/lstm.png\"/>\n",
    " \n",
    "The LSTM above consists of multiple parts that allows it to decide what is important in the short term connection and the long term connection. The above picture instead of showing the connection as a loop it separates it into different state for each item in the sequence which is the same as a loop just easier to look at. The top loop is the long term connection well the bottom loop is the short term connection which is the same as the output of the network. When the network receives an item it uses both the new item and the short term connection to determine what to add and remove from the long term connection. The first step is determining what needs to be removed from the long term connection which is done by the left sigma neural network called the forget gate for example if the long term connection contains information that says the person in the sentence is female so predict female pronouns and then the new item introduces a person who is male then we want to remove predicting female pronouns from the long term memory. The next part is the input gate which is the second sigma neural network and combines with the third part, the tanh neural network, called the candidate network. The input gate determines which of the candidates should be added to the long term connection and the candidate network determines which part of the short term connection and new item are important. The last part is the output network which takes the short term connection and new item and determines what to output which is then adjusted based on the information in the long term connection.\n",
    "\n",
    "The above is quite difficult to understand however thankfully it is easy to determine when to use an LSTM and all that is necessary to know about the LSTM is the regularization methods used to prevent overfitting. LSTM as of now is the best architecture to use for any data that is in a sequence. Going back to AWD-LSTM we now know what the LSTM part is but what about the AWD part well thankfully this part is much simpler. AWD-LSTM stands for Average Stochastic Gradient Descent Weight Dropped Long Short Term Memory, you may notice the only thing left to learn about AWD-LSTM is the Average Stochastic and Weight Dropped parts. Firstly the average stochastic part contains gradient descent as doing gradient descent for every single sample in the dataset is far too slow so instead we randomly choose a sample and get the gradient for that point which is called stochastic gradient descent and then because only using 1 sample to determine how to update the model is a bad idea we take the average of multiple samples. Now we only need to learn about the weight dropped part which is how the model is regularized. RNNs and LSTMs both have a problem of overfitting on the data very quickly due to using data multiple times so to regularize the network we can use techniques like dropout.\n",
    "\n",
    "Dropout has been massively successful in feed forward and convolutional neural networks but recurrent neural networks have a problem with dropout because they cannot hold long term connections if they are dropped. Dropout works by randomly zeroing the activations of some neurons as you are training which cause the neural network to spread its training along multiple neurons instead of making dominant connections between neurons. A better way to apply dropout for recurrent neural networks is using a random sequence to zero different things and then repeating that sequence to provide consistency that does not affect the long term connection.\n",
    "\n",
    "Four different types of dropout are applied called embedding, input, weight and hidden. Embedding dropout zeroes some of the embedding vectors as we are training. An embedding vector is how we transform the text to usable inputs for the neural network by mapping each word to a vector and then training these vectors they should be an accurate representation of words that are related to the input word. Applying embedding dropout simply means we are ignoring some words as we are training which forces the model to learn more about how sentences are structured rather than learning which words are high quality. Input dropout is applied to the result of the embedding so while embedding dropout ignores some words input dropout ignores parts of the word. Weight dropout is simply zeroing some of the weights of a neuron which essentially removes the connection from one neuron to another. The last type of dropout that is applied is called hidden dropout which zeroes the result of a neuron which means that all connections from that neuron are removed. This use of dropout forces the neural network to be more robust but makes the neural network take longer to minimize the loss function.\n",
    "\n",
    "\n",
    "The above should be enough that we can start training our neural network and adjust hyper parameters to optimize the neural network. A neural network has a parameter called learning rate which we can adjust to change how fast a network will learn. We want to reduce the amount of times we learn off the same data as eventually the neural network will learn how to classify each input rather than learning what parameters affect the classification. Choosing an optimal learning rate allows us to minimize the loss function with less data required however choosing a bad learning rate could make your model be unable to minimize the function. The learning rate affects the size of the step along the loss function; The optimal learning rate is important because too large could cause the network to bounce from one side of the minima to the other and too small could cause the network to get stuck in a local minima. You may assume that if we want a learning rate that converges fast and isn't too large then we should decrease our learning rate as we are training. This is close to what is implemented but instead we use an approach called one cycle to adjust the learning rate. One cycle adjusts the learning rate by starting with a low learning rate and increases the learning rate until it hits the maximum learning rate and then decreases the learning rate again. This seems counter intuitive to start with a low learning rate but one cycle adjusts another parameter as well called momentum. Momentum adds part of the previous step into the current step and much like the learning rate we adjust the momentum as we are training however instead of starting with low momentum we instead start with our max momentum. We decrease the momentum while learning rate is increasing and increase momentum while learning rate is decreasing. Momentum has the benefit of helping to avoid local minima however having a high momentum and high learning rate can cause the network to move too fast and step so far away from the minima that it may make it near impossible to get back and restarting is the best option.\n",
    "\n",
    "### Learning Rate too low\n",
    "<img src=\"./images/lowlr.gif\"/>\n",
    "\n",
    "### Good Learning Rate\n",
    "<img src=\"./images/goodlr.gif\"/>\n",
    "\n",
    "### Learning Rate too high\n",
    "<img src=\"./images/highlr.gif\"/>\n",
    "\n",
    "### Learning Rate far too high\n",
    "<img src=\"./images/badlr.gif\"/>\n",
    "\n",
    "There is still lots more about neural networks to learn as this focused only on the basics and then went more specific on the design of the neural network used by KEEP. This should give you the knowledge necessary to understand what is actually happening when you are training the neural network used by KEEP. The best resource I can recommend for learning more is the fastai course at https://course.fast.ai which has two parts that teaches the basics and then the second part is rebuilding the fastai library from scratch to get a more indepth look about the whole process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "132px",
    "width": "402px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "224px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
